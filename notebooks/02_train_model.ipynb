{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Huấn luyện mô hình PMIS\n",
        "\n",
        "Notebook này thực hiện huấn luyện các mô hình ML/NLP cho hệ thống gợi ý thiết bị PMIS\n",
        "\n",
        "## Mục lục\n",
        "- A. Chuẩn bị dữ liệu\n",
        "- B. Data augmentation\n",
        "- C. Xây dựng mô hình\n",
        "- D. Phát hiện lỗi/thiếu\n",
        "- E. Huấn luyện & đánh giá\n",
        "- F. Xuất mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-03 15:14:56.084863: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-03 15:14:56.113700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-02-03 15:14:56.761720: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ sentence-transformers không khả dụng: ValueError\n",
            "Project Root: /home/aispcit/Documents/QuangLV/PMIS v 13\n",
            "Input File: /home/aispcit/Documents/QuangLV/PMIS v 13/data/devicesPMISMayCat_cleaned.csv\n",
            "Model Directory: /home/aispcit/Documents/QuangLV/PMIS v 13/models\n"
          ]
        }
      ],
      "source": [
        "# Import thư viện\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, top_k_accuracy_score\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# NLP libraries\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    HAS_SENTENCE_TRANSFORMERS = True\n",
        "except Exception as e:\n",
        "    HAS_SENTENCE_TRANSFORMERS = False\n",
        "    print(f\"⚠️ sentence-transformers không khả dụng: {type(e).__name__}\")\n",
        "\n",
        "# Thêm đường dẫn project\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "sys.path.append(PROJECT_ROOT)\n",
        "\n",
        "# Đường dẫn\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
        "CONFIG_DIR = os.path.join(PROJECT_ROOT, 'config')\n",
        "LOG_DIR = os.path.join(PROJECT_ROOT, 'logs')\n",
        "\n",
        "# Tạo thư mục nếu chưa có\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# File paths\n",
        "INPUT_FILE = os.path.join(DATA_DIR, 'devicesPMISMayCat_cleaned.csv')\n",
        "\n",
        "print(f\"Project Root: {PROJECT_ROOT}\")\n",
        "print(f\"Input File: {INPUT_FILE}\")\n",
        "print(f\"Model Directory: {MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A. Chuẩn bị dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "A.1. ĐỌC DỮ LIỆU ĐÃ LÀM SẠCH\n",
            "============================================================\n",
            "Đã đọc 1,696 dòng dữ liệu\n",
            "Số cột: 32\n"
          ]
        }
      ],
      "source": [
        "# A.1. Đọc dữ liệu đã làm sạch\n",
        "print(\"=\" * 60)\n",
        "print(\"A.1. ĐỌC DỮ LIỆU ĐÃ LÀM SẠCH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Kiểm tra file tồn tại\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    # Nếu chưa có file cleaned, đọc file gốc\n",
        "    INPUT_FILE = os.path.join(DATA_DIR, 'devicesPMISMayCat.csv')\n",
        "    print(f\"⚠️ File cleaned chưa tồn tại, đọc file gốc: {INPUT_FILE}\")\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE, delimiter=';', encoding='utf-8')\n",
        "print(f\"Đã đọc {len(df):,} dòng dữ liệu\")\n",
        "print(f\"Số cột: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Định nghĩa các quy tắc chuẩn hóa\n",
        "NORMALIZATION_RULES = {\n",
        "    'PHA': 'EVN.PHA_3P',\n",
        "    'KIEU_MC': 'TBI_CT_MC_KIEU_MC_01',\n",
        "    'KIEU_DAPHQ': 'TBI_TT_MC_KIEU_DAPHQ.00001',\n",
        "    'KIEU_CD': 'TBI_CT_MC_CC_CD.00001',\n",
        "    'U_TT': 'TBI_CT_MC_U_TT_02',\n",
        "}\n",
        "FORBIDDEN_NATIONALFACT = 'TB040.00023'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "A.2. KIỂM TRA VÀ ÁP DỤNG QUY TẮC CHUẨN HÓA\n",
            "============================================================\n",
            "PHA: 0 giá trị không chuẩn (expected: EVN.PHA_3P)\n",
            "KIEU_MC: 6 giá trị không chuẩn (expected: TBI_CT_MC_KIEU_MC_01)\n",
            "KIEU_DAPHQ: 11 giá trị không chuẩn (expected: TBI_TT_MC_KIEU_DAPHQ.00001)\n",
            "KIEU_CD: 273 giá trị không chuẩn (expected: TBI_CT_MC_CC_CD.00001)\n",
            "U_TT: 1494 giá trị không chuẩn (expected: TBI_CT_MC_U_TT_02)\n",
            "NATIONALFACT = TB040.00023: 0 bản ghi vi phạm\n"
          ]
        }
      ],
      "source": [
        "# A.2. Áp dụng lại rules chuẩn hóa để đảm bảo 100%\n",
        "print(\"=\" * 60)\n",
        "print(\"A.2. KIỂM TRA VÀ ÁP DỤNG QUY TẮC CHUẨN HÓA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Thay thế 'NULL' string thành NaN\n",
        "df = df.replace('NULL', np.nan)\n",
        "\n",
        "# Kiểm tra các quy tắc\n",
        "for col, expected_value in NORMALIZATION_RULES.items():\n",
        "    if col in df.columns:\n",
        "        non_standard = df[col].notna() & (df[col] != expected_value)\n",
        "        print(f\"{col}: {non_standard.sum()} giá trị không chuẩn (expected: {expected_value})\")\n",
        "\n",
        "# Kiểm tra NATIONALFACT\n",
        "if 'NATIONALFACT' in df.columns:\n",
        "    forbidden = (df['NATIONALFACT'] == FORBIDDEN_NATIONALFACT).sum()\n",
        "    print(f\"NATIONALFACT = {FORBIDDEN_NATIONALFACT}: {forbidden} bản ghi vi phạm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "A.3. CHIA TẬP DỮ LIỆU\n",
            "============================================================\n",
            "Feature columns: 13\n",
            "Text columns: 5\n",
            "\n",
            "So dong sau khi loai bo NaN trong target: 1,696\n",
            "\n",
            "Phan bo LOAI (top 10):\n",
            "  TBI_CT_MC_KIEU_MC.99020: 414 (24.4%)\n",
            "  TBI_CT_MC_KIEU_MC.99019: 357 (21.0%)\n",
            "  TBI_CT_MC_KIEU_MC.99010: 323 (19.0%)\n",
            "  TBI_CT_MC_KIEU_MC.00057: 216 (12.7%)\n",
            "  TBI_CT_MC_KIEU_MC.99001: 81 (4.8%)\n",
            "  TBI_CT_MC_KIEU_MC.00071: 58 (3.4%)\n",
            "  TBI_CT_MC_KIEU_MC.99006: 30 (1.8%)\n",
            "  TBI_CT_MC_KIEU_MC.99018: 25 (1.5%)\n",
            "  TBI_CT_MC_KIEU_MC.99005: 21 (1.2%)\n",
            "  TBI_CT_MC_KIEU_MC.99017: 20 (1.2%)\n",
            "\n",
            "Phan bo P_MANUFACTURERID (top 10):\n",
            "  HSX.00311: 425 (25.1%)\n",
            "  HSX.00046: 366 (21.6%)\n",
            "  HSX.00035: 248 (14.6%)\n",
            "  HSX.00505: 233 (13.7%)\n",
            "  HSX.00051: 121 (7.1%)\n",
            "  HSX.00183: 99 (5.8%)\n",
            "  HSX.00092: 71 (4.2%)\n",
            "  HSX.00508: 31 (1.8%)\n",
            "  HSX.00535: 25 (1.5%)\n",
            "  HSX.00473: 21 (1.2%)\n",
            "\n",
            "Phân chia dữ liệu:\n",
            "  Train: 1,187 (70.0%)\n",
            "  Validation: 254 (15.0%)\n",
            "  Test: 255 (15.0%)\n"
          ]
        }
      ],
      "source": [
        "# A.3. Split train/val/test (70/15/15)\n",
        "print(\"=\" * 60)\n",
        "print(\"A.3. CHIA TẬP DỮ LIỆU\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Định nghĩa các cột features và target\n",
        "# Target: Gợi ý CATEGORYID hoặc các thuộc tính thiết bị\n",
        "TARGET_COLS = ['LOAI', 'P_MANUFACTURERID']\n",
        "\n",
        "# Features: Các cột không phải target và không phải _DESC\n",
        "# Base features - se duoc dieu chinh tuy theo target\n",
        "BASE_FEATURE_COLS = [\n",
        "    'DATEMANUFACTURE', 'NATIONALFACT', 'OWNER',\n",
        "    'U_TT', 'KIEU_DAPHQ', 'I_DM', 'U_DM', 'KIEU_CD',\n",
        "    'TG_CATNM', 'PHA', 'KIEU_MC', 'KNCDNMDM', 'CT_DC'\n",
        "]\n",
        "FEATURE_COLS = BASE_FEATURE_COLS.copy()\n",
        "\n",
        "# Text features (cho NLP)\n",
        "# Text features - loai bo cac _DESC lien quan den target\n",
        "TEXT_COLS = ['ASSETDESC', 'FIELDDESC', 'OWNER_DESC', 'KIEU_MC_DESC', 'KIEU_DAPHQ_DESC']\n",
        "\n",
        "# Lọc các cột tồn tại\n",
        "FEATURE_COLS = [col for col in FEATURE_COLS if col in df.columns]\n",
        "TEXT_COLS = [col for col in TEXT_COLS if col in df.columns]\n",
        "\n",
        "print(f\"Feature columns: {len(FEATURE_COLS)}\")\n",
        "print(f\"Text columns: {len(TEXT_COLS)}\")\n",
        "\n",
        "# Chia dữ liệu\n",
        "# Loai bo cac dong co target la NaN\n",
        "df_clean = df.dropna(subset=TARGET_COLS)\n",
        "print(f\"\\nSo dong sau khi loai bo NaN trong target: {len(df_clean):,}\")\n",
        "\n",
        "# Hien thi phan bo cua cac target\n",
        "for target in TARGET_COLS:\n",
        "    print(f\"\\nPhan bo {target} (top 10):\")\n",
        "    dist = df_clean[target].value_counts().head(10)\n",
        "    for val, count in dist.items():\n",
        "        print(f\"  {val}: {count:,} ({count/len(df_clean)*100:.1f}%)\")\n",
        "\n",
        "train_df, temp_df = train_test_split(df_clean, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"\\nPhân chia dữ liệu:\")\n",
        "print(f\"  Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B.3. Xử lý Class Imbalance\n",
        "\n",
        "> ⚠️ **LƯU Ý:** Các cell B.3.1 đến B.3.3 định nghĩa các hàm. Cell **B.3.4** cần được chạy **SAU** khi đã chạy các cells từ **Section C** (để có `X_train`, `y_train_dict`, `target_encoders`).\n",
        "\n",
        "Áp dụng các kỹ thuật để cải thiện accuracy:\n",
        "1. **SMOTE** - Synthetic Minority Over-sampling Technique\n",
        "2. **Class Weights** - Cân bằng trọng số cho các class nhỏ\n",
        "3. **Minimum Sample Threshold** - Loại bỏ hoặc gộp classes quá nhỏ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "B.3.1. THIẾT LẬP XỬ LÝ CLASS IMBALANCE\n",
            "============================================================\n",
            "✓ imbalanced-learn đã được import thành công\n"
          ]
        }
      ],
      "source": [
        "# B.3.1. Cài đặt và Import thư viện xử lý Class Imbalance\n",
        "print(\"=\" * 60)\n",
        "print(\"B.3.1. THIẾT LẬP XỬ LÝ CLASS IMBALANCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import SMOTE từ imbalanced-learn\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE, ADASYN\n",
        "    from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "    HAS_IMBLEARN = True\n",
        "    print(\"✓ imbalanced-learn đã được import thành công\")\n",
        "except ImportError:\n",
        "    HAS_IMBLEARN = False\n",
        "    print(\"⚠️ imbalanced-learn chưa cài đặt\")\n",
        "    print(\"   Cài đặt: pip install imbalanced-learn\")\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Cấu hình\n",
        "MIN_SAMPLES_PER_CLASS = 5  # Class có ít hơn sẽ được xử lý đặc biệt\n",
        "SMOTE_K_NEIGHBORS = 3      # Số neighbors cho SMOTE (phải < min samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "B.3.2. ĐỊNH NGHĨA HÀM PHÂN TÍCH CLASS IMBALANCE\n",
            "============================================================\n",
            "✓ Hàm analyze_class_imbalance đã được định nghĩa\n",
            "  (Sẽ được gọi sau khi y_train_dict được tạo)\n"
          ]
        }
      ],
      "source": [
        "# B.3.2. Định nghĩa hàm Phân tích Class Imbalance\n",
        "print(\"=\" * 60)\n",
        "print(\"B.3.2. ĐỊNH NGHĨA HÀM PHÂN TÍCH CLASS IMBALANCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def analyze_class_imbalance(y, target_name, encoder=None):\n",
        "    \"\"\"Phân tích chi tiết class imbalance\"\"\"\n",
        "    class_counts = Counter(y)\n",
        "    n_classes = len(class_counts)\n",
        "    total_samples = len(y)\n",
        "    \n",
        "    # Sắp xếp theo số lượng\n",
        "    sorted_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # Thống kê\n",
        "    counts_list = list(class_counts.values())\n",
        "    max_count = max(counts_list)\n",
        "    min_count = min(counts_list)\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "    \n",
        "    print(f\"\\n{target_name}:\")\n",
        "    print(f\"  Tổng samples: {total_samples:,}\")\n",
        "    print(f\"  Số classes: {n_classes}\")\n",
        "    print(f\"  Max samples/class: {max_count}\")\n",
        "    print(f\"  Min samples/class: {min_count}\")\n",
        "    print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
        "    \n",
        "    # Phân loại classes\n",
        "    tiny_classes = [(c, cnt) for c, cnt in sorted_counts if cnt < MIN_SAMPLES_PER_CLASS]\n",
        "    small_classes = [(c, cnt) for c, cnt in sorted_counts if MIN_SAMPLES_PER_CLASS <= cnt < 10]\n",
        "    medium_classes = [(c, cnt) for c, cnt in sorted_counts if 10 <= cnt < 50]\n",
        "    large_classes = [(c, cnt) for c, cnt in sorted_counts if cnt >= 50]\n",
        "    \n",
        "    print(f\"\\n  Phân loại classes:\")\n",
        "    print(f\"    - Tiny (< {MIN_SAMPLES_PER_CLASS} samples): {len(tiny_classes)} classes\")\n",
        "    print(f\"    - Small (5-9 samples): {len(small_classes)} classes\")\n",
        "    print(f\"    - Medium (10-49 samples): {len(medium_classes)} classes\")\n",
        "    print(f\"    - Large (≥ 50 samples): {len(large_classes)} classes\")\n",
        "    \n",
        "    if tiny_classes:\n",
        "        print(f\"\\n  ⚠️ Tiny classes (cần xử lý đặc biệt):\")\n",
        "        for cls_id, cnt in tiny_classes[:10]:\n",
        "            if encoder is not None:\n",
        "                try:\n",
        "                    cls_name = encoder.inverse_transform([cls_id])[0]\n",
        "                except:\n",
        "                    cls_name = cls_id\n",
        "            else:\n",
        "                cls_name = cls_id\n",
        "            print(f\"      {cls_name}: {cnt} samples\")\n",
        "        if len(tiny_classes) > 10:\n",
        "            print(f\"      ... và {len(tiny_classes) - 10} classes khác\")\n",
        "    \n",
        "    return {\n",
        "        'total': total_samples,\n",
        "        'n_classes': n_classes,\n",
        "        'imbalance_ratio': imbalance_ratio,\n",
        "        'tiny_classes': tiny_classes,\n",
        "        'small_classes': small_classes,\n",
        "        'class_counts': class_counts\n",
        "    }\n",
        "\n",
        "print(\"✓ Hàm analyze_class_imbalance đã được định nghĩa\")\n",
        "print(\"  (Sẽ được gọi sau khi y_train_dict được tạo)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "B.3.3. ĐỊNH NGHĨA HÀM XỬ LÝ CLASS IMBALANCE\n",
            "============================================================\n",
            "✓ Các hàm xử lý class imbalance đã được định nghĩa\n"
          ]
        }
      ],
      "source": [
        "# B.3.3. Hàm xử lý Class Imbalance\n",
        "print(\"=\" * 60)\n",
        "print(\"B.3.3. ĐỊNH NGHĨA HÀM XỬ LÝ CLASS IMBALANCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def compute_class_weights(y):\n",
        "    \"\"\"Tính class weights để cân bằng\"\"\"\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    classes = np.unique(y)\n",
        "    weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    return dict(zip(classes, weights))\n",
        "\n",
        "def filter_minority_classes(X, y, min_samples=MIN_SAMPLES_PER_CLASS):\n",
        "    \"\"\"Loại bỏ các samples thuộc class có quá ít mẫu\"\"\"\n",
        "    class_counts = Counter(y)\n",
        "    valid_classes = [c for c, cnt in class_counts.items() if cnt >= min_samples]\n",
        "    \n",
        "    mask = np.isin(y, valid_classes)\n",
        "    X_filtered = X[mask]\n",
        "    y_filtered = y[mask]\n",
        "    \n",
        "    removed_samples = len(y) - len(y_filtered)\n",
        "    removed_classes = len(class_counts) - len(valid_classes)\n",
        "    \n",
        "    print(f\"  Loại bỏ {removed_samples} samples từ {removed_classes} classes có < {min_samples} mẫu\")\n",
        "    \n",
        "    return X_filtered, y_filtered, valid_classes\n",
        "\n",
        "def apply_smote(X, y, k_neighbors=SMOTE_K_NEIGHBORS, strategy='auto'):\n",
        "    \"\"\"Áp dụng SMOTE để oversample minority classes\"\"\"\n",
        "    if not HAS_IMBLEARN:\n",
        "        print(\"  ⚠️ SMOTE không khả dụng (cần cài imbalanced-learn)\")\n",
        "        return X, y\n",
        "    \n",
        "    # Kiểm tra số samples tối thiểu cho SMOTE\n",
        "    class_counts = Counter(y)\n",
        "    min_samples = min(class_counts.values())\n",
        "    \n",
        "    if min_samples <= k_neighbors:\n",
        "        print(f\"  ⚠️ Min class có {min_samples} samples < k_neighbors={k_neighbors}\")\n",
        "        print(f\"  → Điều chỉnh k_neighbors = {min_samples - 1}\")\n",
        "        k_neighbors = max(1, min_samples - 1)\n",
        "    \n",
        "    try:\n",
        "        smote = SMOTE(k_neighbors=k_neighbors, random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "        \n",
        "        print(f\"  SMOTE: {len(y):,} → {len(y_resampled):,} samples\")\n",
        "        \n",
        "        # Hiển thị phân bố mới\n",
        "        new_counts = Counter(y_resampled)\n",
        "        print(f\"  Phân bố sau SMOTE:\")\n",
        "        print(f\"    Min samples/class: {min(new_counts.values())}\")\n",
        "        print(f\"    Max samples/class: {max(new_counts.values())}\")\n",
        "        \n",
        "        return X_resampled, y_resampled\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠️ SMOTE failed: {e}\")\n",
        "        return X, y\n",
        "\n",
        "def prepare_balanced_data(X_train, y_train, target_name, use_smote=True, use_filter=True):\n",
        "    \"\"\"Pipeline xử lý class imbalance hoàn chỉnh\"\"\"\n",
        "    print(f\"\\n  Xử lý class imbalance cho {target_name}:\")\n",
        "    \n",
        "    X_balanced = X_train.copy()\n",
        "    y_balanced = y_train.copy()\n",
        "    valid_classes = np.unique(y_train)\n",
        "    \n",
        "    # Bước 1: Lọc classes quá nhỏ (nếu cần)\n",
        "    if use_filter:\n",
        "        X_balanced, y_balanced, valid_classes = filter_minority_classes(\n",
        "            X_balanced, y_balanced, min_samples=MIN_SAMPLES_PER_CLASS\n",
        "        )\n",
        "    \n",
        "    # Bước 2: Áp dụng SMOTE (nếu có)\n",
        "    if use_smote and HAS_IMBLEARN and len(np.unique(y_balanced)) > 1:\n",
        "        X_balanced, y_balanced = apply_smote(X_balanced, y_balanced)\n",
        "    \n",
        "    # Bước 3: Tính class weights\n",
        "    class_weights = compute_class_weights(y_balanced)\n",
        "    \n",
        "    return X_balanced, y_balanced, class_weights, valid_classes\n",
        "\n",
        "print(\"✓ Các hàm xử lý class imbalance đã được định nghĩa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "B.3.5. CẬP NHẬT MODELS VỚI CLASS WEIGHTS\n",
            "============================================================\n",
            "✓ Hàm create_models_with_weights đã được định nghĩa\n"
          ]
        }
      ],
      "source": [
        "# B.3.5. Cập nhật Models với Class Weights\n",
        "print(\"=\" * 60)\n",
        "print(\"B.3.5. CẬP NHẬT MODELS VỚI CLASS WEIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def create_models_with_weights(class_weights=None):\n",
        "    \"\"\"Tạo models với class weights để xử lý imbalance\"\"\"\n",
        "    \n",
        "    # Model 1: Random Forest với class weights\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=200,           # Tăng số cây\n",
        "        max_depth=15,               # Tăng độ sâu\n",
        "        min_samples_split=5,        # Tránh overfit\n",
        "        min_samples_leaf=2,\n",
        "        class_weight=class_weights, # SỬ DỤNG CLASS WEIGHTS\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Model 2: Gradient Boosting (không hỗ trợ class_weight trực tiếp)\n",
        "    gb_model = GradientBoostingClassifier(\n",
        "        n_estimators=150,           # Tăng số iterations\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Model 3: KNN (sử dụng weights='distance')\n",
        "    knn_model = KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        weights='distance',\n",
        "        metric='cosine'\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'RandomForest': rf_model,\n",
        "        'GradientBoosting': gb_model,\n",
        "        'KNN': knn_model\n",
        "    }\n",
        "\n",
        "print(\"✓ Hàm create_models_with_weights đã được định nghĩa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "B.1. KIỂM TRA PHÂN BỐ DỮ LIỆU\n",
            "============================================================\n",
            "\n",
            "Phan bo LOAI (top 10):\n",
            "  TBI_CT_MC_KIEU_MC.99020: 414 (24.4%)\n",
            "  TBI_CT_MC_KIEU_MC.99019: 357 (21.0%)\n",
            "  TBI_CT_MC_KIEU_MC.99010: 323 (19.0%)\n",
            "  TBI_CT_MC_KIEU_MC.00057: 216 (12.7%)\n",
            "  TBI_CT_MC_KIEU_MC.99001: 81 (4.8%)\n",
            "  TBI_CT_MC_KIEU_MC.00071: 58 (3.4%)\n",
            "  TBI_CT_MC_KIEU_MC.99006: 30 (1.8%)\n",
            "  TBI_CT_MC_KIEU_MC.99018: 25 (1.5%)\n",
            "  TBI_CT_MC_KIEU_MC.99005: 21 (1.2%)\n",
            "  TBI_CT_MC_KIEU_MC.99017: 20 (1.2%)\n",
            "\n",
            "Phan bo P_MANUFACTURERID (top 10):\n",
            "  HSX.00311: 425 (25.1%)\n",
            "  HSX.00046: 366 (21.6%)\n",
            "  HSX.00035: 248 (14.6%)\n",
            "  HSX.00505: 233 (13.7%)\n",
            "  HSX.00051: 121 (7.1%)\n",
            "  HSX.00183: 99 (5.8%)\n",
            "  HSX.00092: 71 (4.2%)\n",
            "  HSX.00508: 31 (1.8%)\n",
            "  HSX.00535: 25 (1.5%)\n",
            "  HSX.00473: 21 (1.2%)\n",
            "\n",
            "Phân bố nhà sản xuất (top 10):\n",
            "  HSX.00311: 425\n",
            "  HSX.00046: 366\n",
            "  HSX.00035: 248\n",
            "  HSX.00505: 233\n",
            "  HSX.00051: 121\n",
            "  HSX.00183: 99\n",
            "  HSX.00092: 71\n",
            "  HSX.00508: 31\n",
            "  HSX.00535: 25\n",
            "  HSX.00473: 21\n"
          ]
        }
      ],
      "source": [
        "# B.1. Kiểm tra phân bố dữ liệu\n",
        "print(\"=\" * 60)\n",
        "print(\"B.1. KIỂM TRA PHÂN BỐ DỮ LIỆU\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Phân bố theo CATEGORYID\n",
        "for target in TARGET_COLS:\n",
        "    if target in df.columns:\n",
        "        print(f\"\\nPhan bo {target} (top 10):\")\n",
        "        dist = df[target].value_counts().head(10)\n",
        "        for val, count in dist.items():\n",
        "            print(f\"  {val}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Phân bố theo nhà sản xuất\n",
        "if 'P_MANUFACTURERID' in df.columns:\n",
        "    print(f\"\\nPhân bố nhà sản xuất (top 10):\")\n",
        "    mfg_dist = df['P_MANUFACTURERID'].value_counts().head(10)\n",
        "    for mfg, count in mfg_dist.items():\n",
        "        print(f\"  {mfg}: {count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "B.2. DATA AUGMENTATION\n",
            "============================================================\n",
            "\n",
            "LOAI - Cac class co it hon 10 samples:\n",
            "  PB-500158: 8\n",
            "  TBI_CT_MC_KIEU_MC.99003: 7\n",
            "  TBI_CT_MC_KIEU_MC.99004: 7\n",
            "  TBI_CT_MC_KIEU_MC.000102: 6\n",
            "  TBI_CT_MC_KIEU_MC.00069: 6\n",
            "  TBI_CT_MC_KIEU_MC.00055: 5\n",
            "  TBI_CT_MC_KIEU_MC.99034: 4\n",
            "  TBI_CT_MC_KIEU_MC.99035: 3\n",
            "  TBI_CT_MC_KIEU_MC.99002: 3\n",
            "  TBI_CT_MC_KIEU_MC.99032: 2\n",
            "  TBI_CT_MC_KIEU_MC.99015: 2\n",
            "  TBI_CT_MC_KIEU_MC.99029: 2\n",
            "  TBI_CT_MC_KIEU_MC.99031: 2\n",
            "  TBI_CT_MC_KIEU_MC.99026: 2\n",
            "  TBI_CT_MC_KIEU_MC.99030: 2\n",
            "  TBI_CT_MC_KIEU_MC.99033: 1\n",
            "  TBI_CT_MC_KIEU_MC.99027: 1\n",
            "  TBI_CT_MC_KIEU_MC.99036: 1\n",
            "  TBI_CT_MC_KIEU_MC.99012: 1\n",
            "\n",
            "P_MANUFACTURERID - Cac class co it hon 10 samples:\n",
            "  HSX.00417: 9\n",
            "  HSX.00513: 5\n",
            "  HSX.00544: 4\n",
            "  PB-100056: 3\n",
            "  HSX.T.662: 3\n",
            "  HSX.T.700: 2\n",
            "  HSX.T.704: 2\n",
            "  HSX.00203: 2\n",
            "  HSX.00529: 1\n",
            "  HSX.00299: 1\n",
            "  HSX.00290: 1\n",
            "  PB-100109: 1\n",
            "  HSX.00507: 1\n",
            "  PB-100016: 1\n",
            "  HSX.T.687: 1\n",
            "  HSX.00184: 1\n",
            "\n",
            "⚠️ Cần data augmentation cho các class nhỏ\n"
          ]
        }
      ],
      "source": [
        "# B.2. Synthetic data generation (nếu cần)\n",
        "print(\"=\" * 60)\n",
        "print(\"B.2. DATA AUGMENTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def augment_text_data(df, text_col, num_augments=2):\n",
        "    \"\"\"\n",
        "    Tạo synthetic samples bằng cách biến đổi text\n",
        "    - Thêm/bỏ khoảng trắng\n",
        "    - Đổi chữ hoa/thường\n",
        "    - Thêm typo nhẹ\n",
        "    \"\"\"\n",
        "    augmented_rows = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.notna(row[text_col]):\n",
        "            text = str(row[text_col])\n",
        "            \n",
        "            # Variation 1: Upper case\n",
        "            new_row = row.copy()\n",
        "            new_row[text_col] = text.upper()\n",
        "            augmented_rows.append(new_row)\n",
        "            \n",
        "            # Variation 2: Lower case\n",
        "            new_row = row.copy()\n",
        "            new_row[text_col] = text.lower()\n",
        "            augmented_rows.append(new_row)\n",
        "    \n",
        "    return pd.DataFrame(augmented_rows)\n",
        "\n",
        "# Kiểm tra nếu cần augmentation\n",
        "MIN_SAMPLES_PER_CLASS = 10\n",
        "need_augmentation = False\n",
        "\n",
        "# Kiem tra cho tung target\n",
        "for target in TARGET_COLS:\n",
        "    if target in train_df.columns:\n",
        "        class_counts = train_df[target].value_counts()\n",
        "        small_classes = class_counts[class_counts < MIN_SAMPLES_PER_CLASS]\n",
        "        if len(small_classes) > 0:\n",
        "            need_augmentation = True\n",
        "            print(f\"\\n{target} - Cac class co it hon {MIN_SAMPLES_PER_CLASS} samples:\")\n",
        "            for cls, count in small_classes.items():\n",
        "                print(f\"  {cls}: {count}\")\n",
        "\n",
        "if need_augmentation and 'ASSETDESC' in train_df.columns:\n",
        "    print(\"\\n⚠️ Cần data augmentation cho các class nhỏ\")\n",
        "    # Uncomment để thực hiện augmentation\n",
        "    # augmented_df = augment_text_data(train_df[train_df[TARGET_COL].isin(small_classes.index)], 'ASSETDESC')\n",
        "    # train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
        "    # print(f\"Sau augmentation: {len(train_df):,} samples\")\n",
        "else:\n",
        "    print(\"✓ Không cần data augmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Xây dựng mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "C.1. CHUẨN BỊ ENCODERS\n",
            "============================================================\n",
            "NATIONALFACT: 11 classes\n",
            "OWNER: 2 classes\n",
            "U_TT: 5 classes\n",
            "KIEU_DAPHQ: 2 classes\n",
            "I_DM: 5 classes\n",
            "U_DM: 5 classes\n",
            "KIEU_CD: 2 classes\n",
            "TG_CATNM: 2 classes\n",
            "PHA: 1 classes\n",
            "KIEU_MC: 3 classes\n",
            "KNCDNMDM: 5 classes\n",
            "CT_DC: 1 classes\n",
            "TF-IDF ASSETDESC: 100 features\n",
            "TF-IDF FIELDDESC: 23 features\n",
            "TF-IDF OWNER_DESC: 6 features\n",
            "TF-IDF KIEU_MC_DESC: 3 features\n",
            "TF-IDF KIEU_DAPHQ_DESC: 4 features\n"
          ]
        }
      ],
      "source": [
        "# C.1. Chuẩn bị encoders\n",
        "print(\"=\" * 60)\n",
        "print(\"C.1. CHUẨN BỊ ENCODERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Label encoders cho các cột categorical\n",
        "label_encoders = {}\n",
        "for col in FEATURE_COLS:\n",
        "    if df[col].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        # Fit trên toàn bộ dữ liệu để bao gồm tất cả categories\n",
        "        all_values = df[col].fillna('_MISSING_').astype(str).unique()\n",
        "        le.fit(all_values)\n",
        "        label_encoders[col] = le\n",
        "        print(f\"{col}: {len(le.classes_)} classes\")\n",
        "\n",
        "# TF-IDF cho text columns\n",
        "tfidf_vectorizers = {}\n",
        "for col in TEXT_COLS:\n",
        "    tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
        "    text_data = df[col].fillna('').astype(str)\n",
        "    tfidf.fit(text_data)\n",
        "    tfidf_vectorizers[col] = tfidf\n",
        "    print(f\"TF-IDF {col}: {len(tfidf.get_feature_names_out())} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "C.2. TẠO FEATURE MATRIX\n",
            "============================================================\n",
            "X_train shape: (1187, 149)\n",
            "X_val shape: (254, 149)\n",
            "X_test shape: (255, 149)\n",
            "Total features: 149\n"
          ]
        }
      ],
      "source": [
        "# C.2. Tạo feature matrix\n",
        "print(\"=\" * 60)\n",
        "print(\"C.2. TẠO FEATURE MATRIX\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def create_features(df, label_encoders, tfidf_vectorizers):\n",
        "    \"\"\"Tạo feature matrix từ DataFrame\"\"\"\n",
        "    features_list = []\n",
        "    feature_names = []\n",
        "    \n",
        "    # Categorical features\n",
        "    for col, le in label_encoders.items():\n",
        "        values = df[col].fillna('_MISSING_').astype(str)\n",
        "        # Handle unseen labels\n",
        "        encoded = []\n",
        "        for v in values:\n",
        "            if v in le.classes_:\n",
        "                encoded.append(le.transform([v])[0])\n",
        "            else:\n",
        "                encoded.append(-1)  # Unknown\n",
        "        features_list.append(np.array(encoded).reshape(-1, 1))\n",
        "        feature_names.append(col)\n",
        "    \n",
        "    # Text features (TF-IDF)\n",
        "    for col, tfidf in tfidf_vectorizers.items():\n",
        "        text_data = df[col].fillna('').astype(str)\n",
        "        tfidf_features = tfidf.transform(text_data).toarray()\n",
        "        features_list.append(tfidf_features)\n",
        "        feature_names.extend([f\"{col}_tfidf_{i}\" for i in range(tfidf_features.shape[1])])\n",
        "    \n",
        "    # Numeric features\n",
        "    if 'DATEMANUFACTURE' in df.columns:\n",
        "        date_feat = df['DATEMANUFACTURE'].fillna(df['DATEMANUFACTURE'].median()).values.reshape(-1, 1)\n",
        "        features_list.append(date_feat)\n",
        "        feature_names.append('DATEMANUFACTURE')\n",
        "    \n",
        "    # Concatenate all features\n",
        "    X = np.hstack(features_list)\n",
        "    \n",
        "    return X, feature_names\n",
        "\n",
        "# Tạo features cho train/val/test\n",
        "X_train, feature_names = create_features(train_df, label_encoders, tfidf_vectorizers)\n",
        "X_val, _ = create_features(val_df, label_encoders, tfidf_vectorizers)\n",
        "X_test, _ = create_features(test_df, label_encoders, tfidf_vectorizers)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"Total features: {len(feature_names)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "C.3. CHUẨN BỊ TARGET VARIABLE\n",
            "============================================================\n",
            "\n",
            "LOAI:\n",
            "  Number of classes: 34\n",
            "  Classes (top 10): ['PB-500017', 'PB-500158', 'TBI_CT_MC_KIEU_MC.000102', 'TBI_CT_MC_KIEU_MC.00055', 'TBI_CT_MC_KIEU_MC.00057', 'TBI_CT_MC_KIEU_MC.00069', 'TBI_CT_MC_KIEU_MC.00071', 'TBI_CT_MC_KIEU_MC.99001', 'TBI_CT_MC_KIEU_MC.99002', 'TBI_CT_MC_KIEU_MC.99003']...\n",
            "\n",
            "P_MANUFACTURERID:\n",
            "  Number of classes: 28\n",
            "  Classes (top 10): ['HSX.00029', 'HSX.00035', 'HSX.00046', 'HSX.00051', 'HSX.00092', 'HSX.00183', 'HSX.00184', 'HSX.00203', 'HSX.00290', 'HSX.00299']...\n"
          ]
        }
      ],
      "source": [
        "# C.3. Chuẩn bị target variable\n",
        "print(\"=\" * 60)\n",
        "print(\"C.3. CHUẨN BỊ TARGET VARIABLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Encode target\n",
        "# Encode targets - tao encoder cho moi target\n",
        "target_encoders = {}\n",
        "y_train_dict = {}\n",
        "y_val_dict = {}\n",
        "y_test_dict = {}\n",
        "\n",
        "for target in TARGET_COLS:\n",
        "    le = LabelEncoder()\n",
        "    # Fit tren TOAN BO du lieu (df_clean) de bao gom tat ca cac gia tri\n",
        "    le.fit(df_clean[target].fillna('_UNKNOWN_'))\n",
        "    y_train_dict[target] = le.transform(train_df[target].fillna('_UNKNOWN_'))\n",
        "    y_val_dict[target] = le.transform(val_df[target].fillna('_UNKNOWN_'))\n",
        "    y_test_dict[target] = le.transform(test_df[target].fillna('_UNKNOWN_'))\n",
        "    target_encoders[target] = le\n",
        "\n",
        "for target, le in target_encoders.items():\n",
        "    print(f\"\\n{target}:\")\n",
        "    print(f\"  Number of classes: {len(le.classes_)}\")\n",
        "    if len(le.classes_) <= 10:\n",
        "        print(f\"  Classes: {list(le.classes_)}\")\n",
        "    else:\n",
        "        print(f\"  Classes (top 10): {list(le.classes_[:10])}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "C.4. XÂY DỰNG ML MODEL\n",
            "============================================================\n",
            "Đã định nghĩa 3 mô hình:\n",
            "  - RandomForest\n",
            "  - GradientBoosting\n",
            "  - KNN\n"
          ]
        }
      ],
      "source": [
        "# C.4. Xây dựng ML Model (gợi ý thiết bị)\n",
        "print(\"=\" * 60)\n",
        "print(\"C.4. XÂY DỰNG ML MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Model 1: Random Forest\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Model 2: Gradient Boosting\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Model 3: KNN (cho similarity search)\n",
        "knn_model = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='cosine'\n",
        ")\n",
        "\n",
        "models = {\n",
        "    'RandomForest': rf_model,\n",
        "    'GradientBoosting': gb_model,\n",
        "    'KNN': knn_model\n",
        "}\n",
        "\n",
        "print(f\"Đã định nghĩa {len(models)} mô hình:\")\n",
        "for name in models.keys():\n",
        "    print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "C.5. NLP MODEL (CHUẨN HÓA TEXT)\n",
            "============================================================\n",
            "✓ TextNormalizer đã được khởi tạo\n",
            "\n",
            "Test normalization:\n",
            "  '  mc 171  ' -> 'MC 171'\n",
            "  'máy cắt 172' -> 'MÁY CẮT 172'\n",
            "  'MC-132_DSO' -> 'MC-132_DSO'\n"
          ]
        }
      ],
      "source": [
        "# C.5. NLP Model (chuẩn hóa text)\n",
        "print(\"=\" * 60)\n",
        "print(\"C.5. NLP MODEL (CHUẨN HÓA TEXT)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class TextNormalizer:\n",
        "    \"\"\"Chuẩn hóa text input từ OCR\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.replacements = {\n",
        "            # Các từ viết tắt phổ biến\n",
        "            'MC': 'Máy cắt',\n",
        "            'MBA': 'Máy biến áp',\n",
        "            'TI': 'Máy biến dòng',\n",
        "            'TU': 'Máy biến điện áp',\n",
        "            'DCL': 'Dao cách ly',\n",
        "        }\n",
        "        \n",
        "    def normalize(self, text):\n",
        "        \"\"\"Chuẩn hóa một chuỗi text\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return ''\n",
        "        \n",
        "        text = str(text).strip()\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "        # Upper case for consistency\n",
        "        text = text.upper()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def normalize_batch(self, texts):\n",
        "        \"\"\"Chuẩn hóa một batch texts\"\"\"\n",
        "        return [self.normalize(t) for t in texts]\n",
        "\n",
        "text_normalizer = TextNormalizer()\n",
        "print(\"✓ TextNormalizer đã được khởi tạo\")\n",
        "\n",
        "# Test\n",
        "test_texts = ['  mc 171  ', 'máy cắt 172', 'MC-132_DSO']\n",
        "print(f\"\\nTest normalization:\")\n",
        "for t in test_texts:\n",
        "    print(f\"  '{t}' -> '{text_normalizer.normalize(t)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "C.6. SENTENCE EMBEDDINGS\n",
            "============================================================\n",
            "⚠️ sentence-transformers không khả dụng\n",
            "Cài đặt: pip install sentence-transformers\n"
          ]
        }
      ],
      "source": [
        "# C.6. Sentence Embeddings (nếu có)\n",
        "print(\"=\" * 60)\n",
        "print(\"C.6. SENTENCE EMBEDDINGS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if HAS_SENTENCE_TRANSFORMERS:\n",
        "    # Load pre-trained model\n",
        "    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "    print(\"✓ Loaded sentence-transformers model\")\n",
        "    \n",
        "    # Tạo embeddings cho ASSETDESC\n",
        "    if 'ASSETDESC' in df.columns:\n",
        "        sample_texts = df['ASSETDESC'].head(5).fillna('').tolist()\n",
        "        sample_embeddings = embedding_model.encode(sample_texts)\n",
        "        print(f\"Sample embedding shape: {sample_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"⚠️ sentence-transformers không khả dụng\")\n",
        "    print(\"Cài đặt: pip install sentence-transformers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E.0 Multi-Target Training (LOAI va P_MANUFACTURERID)\n",
        "\n",
        "Cell nay thay the cac buoc E.1, E.2, E.3 cu de train models cho nhieu targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "E.0. MULTI-TARGET TRAINING\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TARGET: LOAI\n",
            "============================================================\n",
            "Number of classes: 34\n",
            "\n",
            "  Training RandomForest...\n",
            "    Done in 0.09s\n",
            "\n",
            "  Training GradientBoosting...\n",
            "    Done in 7.97s\n",
            "\n",
            "  Training KNN...\n",
            "    Done in 0.00s\n",
            "\n",
            "  VALIDATION RESULTS:\n",
            "    RandomForest: Accuracy = 0.6535\n",
            "    GradientBoosting: Accuracy = 0.7165\n",
            "    KNN: Accuracy = 0.6142\n",
            "\n",
            "  Best model: GradientBoosting\n",
            "\n",
            "  TEST RESULTS (GradientBoosting):\n",
            "    Accuracy:  0.7490\n",
            "    Precision: 0.7508\n",
            "    Recall:    0.7490\n",
            "    F1-Score:  0.7457\n",
            "    Top-1 Accuracy: 0.7520\n",
            "    Top-3 Accuracy: 0.9134\n",
            "    Top-5 Accuracy: 0.9409\n",
            "\n",
            "============================================================\n",
            "TARGET: P_MANUFACTURERID\n",
            "============================================================\n",
            "Number of classes: 28\n",
            "\n",
            "  Training RandomForest...\n",
            "    Done in 0.10s\n",
            "\n",
            "  Training GradientBoosting...\n",
            "    Done in 6.47s\n",
            "\n",
            "  Training KNN...\n",
            "    Done in 0.00s\n",
            "\n",
            "  VALIDATION RESULTS:\n",
            "    RandomForest: Accuracy = 0.6811\n",
            "    GradientBoosting: Accuracy = 0.7913\n",
            "    KNN: Accuracy = 0.6417\n",
            "\n",
            "  Best model: GradientBoosting\n",
            "\n",
            "  TEST RESULTS (GradientBoosting):\n",
            "    Accuracy:  0.8118\n",
            "    Precision: 0.8307\n",
            "    Recall:    0.8118\n",
            "    F1-Score:  0.8106\n",
            "    Top-1 Accuracy: 0.8182\n",
            "    Top-3 Accuracy: 0.9368\n",
            "    Top-5 Accuracy: 0.9723\n",
            "\n",
            "============================================================\n",
            "HOAN TAT MULTI-TARGET TRAINING\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# E.0. MULTI-TARGET TRAINING\n",
        "print(\"=\" * 60)\n",
        "print(\"E.0. MULTI-TARGET TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import time\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Luu ket qua cho moi target\n",
        "all_results = {}\n",
        "\n",
        "for target in TARGET_COLS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TARGET: {target}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Lay y values cho target hien tai\n",
        "    y_train = y_train_dict[target]\n",
        "    y_val = y_val_dict[target]\n",
        "    y_test = y_test_dict[target]\n",
        "    target_encoder = target_encoders[target]\n",
        "    \n",
        "    n_classes = len(target_encoder.classes_)\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "    \n",
        "    # Train models\n",
        "    target_models = {}\n",
        "    target_times = {}\n",
        "    \n",
        "    for name, model_template in models.items():\n",
        "        model = clone(model_template)\n",
        "        print(f\"\\n  Training {name}...\")\n",
        "        start = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        elapsed = time.time() - start\n",
        "        target_models[name] = model\n",
        "        target_times[name] = elapsed\n",
        "        print(f\"    Done in {elapsed:.2f}s\")\n",
        "    \n",
        "    # Validation\n",
        "    print(f\"\\n  VALIDATION RESULTS:\")\n",
        "    val_results = {}\n",
        "    for name, model in target_models.items():\n",
        "        y_pred = model.predict(X_val)\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        val_results[name] = acc\n",
        "        print(f\"    {name}: Accuracy = {acc:.4f}\")\n",
        "    \n",
        "    # Chon best model\n",
        "    best_name = max(val_results, key=lambda x: val_results[x])\n",
        "    best_model = target_models[best_name]\n",
        "    print(f\"\\n  Best model: {best_name}\")\n",
        "    \n",
        "    # Test evaluation\n",
        "    y_pred_test = best_model.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    test_prec = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "    test_rec = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "    test_f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "    \n",
        "    print(f\"\\n  TEST RESULTS ({best_name}):\")\n",
        "    print(f\"    Accuracy:  {test_acc:.4f}\")\n",
        "    print(f\"    Precision: {test_prec:.4f}\")\n",
        "    print(f\"    Recall:    {test_rec:.4f}\")\n",
        "    print(f\"    F1-Score:  {test_f1:.4f}\")\n",
        "    \n",
        "    # Top-K Accuracy (chi cho multi-class)\n",
        "    if n_classes > 2 and hasattr(best_model, 'predict_proba'):\n",
        "        y_proba = best_model.predict_proba(X_test)\n",
        "        # So classes tu model (co the it hon encoder neu train set khong co tat ca classes)\n",
        "        model_n_classes = y_proba.shape[1]\n",
        "        # Chi tinh top-k neu model co du classes\n",
        "        if model_n_classes >= 3:\n",
        "            # Loc y_test chi giu cac samples co class trong model\n",
        "            model_classes = best_model.classes_\n",
        "            valid_mask = np.isin(y_test, model_classes)\n",
        "            if valid_mask.sum() > 0:\n",
        "                y_test_filtered = y_test[valid_mask]\n",
        "                y_proba_filtered = y_proba[valid_mask]\n",
        "                labels = model_classes\n",
        "                for k in [1, 3, 5]:\n",
        "                    if k <= model_n_classes:\n",
        "                        try:\n",
        "                            top_k = top_k_accuracy_score(y_test_filtered, y_proba_filtered, k=k, labels=labels)\n",
        "                            print(f\"    Top-{k} Accuracy: {top_k:.4f}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"    Top-{k} Accuracy: N/A ({type(e).__name__})\")\n",
        "    \n",
        "    # Luu ket qua\n",
        "    all_results[target] = {\n",
        "        'best_model_name': best_name,\n",
        "        'best_model': best_model,\n",
        "        'all_models': target_models,\n",
        "        'target_encoder': target_encoder,\n",
        "        'metrics': {\n",
        "            'accuracy': test_acc,\n",
        "            'precision': test_prec,\n",
        "            'recall': test_rec,\n",
        "            'f1': test_f1\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"HOAN TAT MULTI-TARGET TRAINING\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "F.0. LUU MULTI-TARGET MODELS\n",
            "============================================================\n",
            "\n",
            "LOAI:\n",
            "  Model: /home/aispcit/Documents/QuangLV/PMIS v 13/models/loai_classifier_20260203_151512.pkl\n",
            "  Encoder: /home/aispcit/Documents/QuangLV/PMIS v 13/models/loai_encoder_20260203_151512.pkl\n",
            "\n",
            "P_MANUFACTURERID:\n",
            "  Model: /home/aispcit/Documents/QuangLV/PMIS v 13/models/p_manufacturerid_classifier_20260203_151512.pkl\n",
            "  Encoder: /home/aispcit/Documents/QuangLV/PMIS v 13/models/p_manufacturerid_encoder_20260203_151512.pkl\n",
            "\n",
            "Feature encoders: /home/aispcit/Documents/QuangLV/PMIS v 13/models/feature_encoders_20260203_151512.pkl\n",
            "\n",
            "Config: /home/aispcit/Documents/QuangLV/PMIS v 13/config/multi_target_config_20260203_151512.json\n",
            "\n",
            "DONE!\n"
          ]
        }
      ],
      "source": [
        "# F.0. LUU MULTI-TARGET MODELS\n",
        "print(\"=\" * 60)\n",
        "print(\"F.0. LUU MULTI-TARGET MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Luu models cho moi target\n",
        "for target, result in all_results.items():\n",
        "    print(f\"\\n{target}:\")\n",
        "    \n",
        "    # Luu best model\n",
        "    model_path = os.path.join(MODEL_DIR, f'{target.lower()}_classifier_{timestamp}.pkl')\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump(result['best_model'], f)\n",
        "    print(f\"  Model: {model_path}\")\n",
        "    \n",
        "    # Luu target encoder\n",
        "    enc_path = os.path.join(MODEL_DIR, f'{target.lower()}_encoder_{timestamp}.pkl')\n",
        "    with open(enc_path, 'wb') as f:\n",
        "        pickle.dump(result['target_encoder'], f)\n",
        "    print(f\"  Encoder: {enc_path}\")\n",
        "\n",
        "# Luu label encoders va TF-IDF\n",
        "encoders_path = os.path.join(MODEL_DIR, f'feature_encoders_{timestamp}.pkl')\n",
        "with open(encoders_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'label_encoders': label_encoders,\n",
        "        'tfidf_vectorizers': tfidf_vectorizers\n",
        "    }, f)\n",
        "print(f\"\\nFeature encoders: {encoders_path}\")\n",
        "\n",
        "# Luu config\n",
        "config = {\n",
        "    'timestamp': timestamp,\n",
        "    'targets': TARGET_COLS,\n",
        "    'feature_columns': FEATURE_COLS,\n",
        "    'text_columns': TEXT_COLS,\n",
        "    'results': {}\n",
        "}\n",
        "\n",
        "for target, result in all_results.items():\n",
        "    config['results'][target] = {\n",
        "        'best_model': result['best_model_name'],\n",
        "        'n_classes': len(result['target_encoder'].classes_),\n",
        "        'metrics': result['metrics']\n",
        "    }\n",
        "\n",
        "config_path = os.path.join(CONFIG_DIR, f'multi_target_config_{timestamp}.json')\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
        "print(f\"\\nConfig: {config_path}\")\n",
        "\n",
        "print(\"\\nDONE!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TONG KET MULTI-TARGET CLASSIFICATION\n",
            "============================================================\n",
            "\n",
            "Targets: ['LOAI', 'P_MANUFACTURERID']\n",
            "Training samples: 1,187\n",
            "Test samples: 255\n",
            "\n",
            "KET QUA:\n",
            "\n",
            "  LOAI:\n",
            "    Best Model: GradientBoosting\n",
            "    Classes: 34\n",
            "    Accuracy: 0.7490\n",
            "    F1-Score: 0.7457\n",
            "\n",
            "  P_MANUFACTURERID:\n",
            "    Best Model: GradientBoosting\n",
            "    Classes: 28\n",
            "    Accuracy: 0.8118\n",
            "    F1-Score: 0.8106\n"
          ]
        }
      ],
      "source": [
        "# TONG KET\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TONG KET MULTI-TARGET CLASSIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTargets: {TARGET_COLS}\")\n",
        "print(f\"Training samples: {len(train_df):,}\")\n",
        "print(f\"Test samples: {len(test_df):,}\")\n",
        "\n",
        "print(f\"\\nKET QUA:\")\n",
        "for target, result in all_results.items():\n",
        "    m = result['metrics']\n",
        "    print(f\"\\n  {target}:\")\n",
        "    print(f\"    Best Model: {result['best_model_name']}\")\n",
        "    print(f\"    Classes: {len(result['target_encoder'].classes_)}\")\n",
        "    print(f\"    Accuracy: {m['accuracy']:.4f}\")\n",
        "    print(f\"    F1-Score: {m['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Phát hiện lỗi/thiếu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "D.1. RULE-BASED DETECTION\n",
            "============================================================\n",
            "Số bản ghi có lỗi (sample 100): 92\n",
            "\n",
            "Ví dụ lỗi phát hiện:\n",
            "  ASSETID: PB-0110D00-MC17716798\n",
            "    - KIEU_CD: non_standard_value\n",
            "    - U_TT: non_standard_value\n",
            "  ASSETID: PB-0110D00-MC15722842\n",
            "    - U_TT: non_standard_value\n",
            "  ASSETID: PB-0110D00-MC12647793\n",
            "    - U_TT: non_standard_value\n"
          ]
        }
      ],
      "source": [
        "# D.1. Rule-based detection\n",
        "print(\"=\" * 60)\n",
        "print(\"D.1. RULE-BASED DETECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class RuleBasedDetector:\n",
        "    \"\"\"Phát hiện lỗi/thiếu dựa trên quy tắc\"\"\"\n",
        "    \n",
        "    def __init__(self, rules, forbidden_values):\n",
        "        self.rules = rules\n",
        "        self.forbidden_values = forbidden_values\n",
        "    \n",
        "    def detect(self, df):\n",
        "        \"\"\"Phát hiện các bản ghi có lỗi\"\"\"\n",
        "        errors = []\n",
        "        \n",
        "        for idx, row in df.iterrows():\n",
        "            row_errors = []\n",
        "            \n",
        "            # Check required fields\n",
        "            for col, expected in self.rules.items():\n",
        "                if col in row.index:\n",
        "                    if pd.notna(row[col]) and row[col] != expected:\n",
        "                        row_errors.append({\n",
        "                            'column': col,\n",
        "                            'type': 'non_standard_value',\n",
        "                            'current': row[col],\n",
        "                            'expected': expected\n",
        "                        })\n",
        "            \n",
        "            # Check forbidden values\n",
        "            for col, forbidden in self.forbidden_values.items():\n",
        "                if col in row.index:\n",
        "                    if row[col] == forbidden:\n",
        "                        row_errors.append({\n",
        "                            'column': col,\n",
        "                            'type': 'forbidden_value',\n",
        "                            'current': row[col]\n",
        "                        })\n",
        "            \n",
        "            # Check missing critical fields\n",
        "            critical_fields = ['ASSETID', 'ASSETDESC', 'CATEGORYID']\n",
        "            for col in critical_fields:\n",
        "                if col in row.index and pd.isna(row[col]):\n",
        "                    row_errors.append({\n",
        "                        'column': col,\n",
        "                        'type': 'missing_critical',\n",
        "                        'current': None\n",
        "                    })\n",
        "            \n",
        "            if row_errors:\n",
        "                errors.append({\n",
        "                    'index': idx,\n",
        "                    'assetid': row.get('ASSETID', 'N/A'),\n",
        "                    'errors': row_errors\n",
        "                })\n",
        "        \n",
        "        return errors\n",
        "\n",
        "# Khởi tạo detector\n",
        "rule_detector = RuleBasedDetector(\n",
        "    rules=NORMALIZATION_RULES,\n",
        "    forbidden_values={'NATIONALFACT': FORBIDDEN_NATIONALFACT}\n",
        ")\n",
        "\n",
        "# Chạy detection trên test set\n",
        "detected_errors = rule_detector.detect(test_df.head(100))\n",
        "print(f\"Số bản ghi có lỗi (sample 100): {len(detected_errors)}\")\n",
        "\n",
        "if detected_errors:\n",
        "    print(f\"\\nVí dụ lỗi phát hiện:\")\n",
        "    for err in detected_errors[:3]:\n",
        "        print(f\"  ASSETID: {err['assetid']}\")\n",
        "        for e in err['errors'][:2]:\n",
        "            print(f\"    - {e['column']}: {e['type']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "D.2. ML-BASED ANOMALY DETECTION\n",
            "============================================================\n",
            "Số anomalies phát hiện: 14 (5.5%)\n"
          ]
        }
      ],
      "source": [
        "# D.2. ML-based anomaly detection\n",
        "print(\"=\" * 60)\n",
        "print(\"D.2. ML-BASED ANOMALY DETECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Sử dụng Isolation Forest\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.05,  # 5% dữ liệu được coi là anomaly\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit và predict\n",
        "iso_forest.fit(X_train)\n",
        "anomaly_scores = iso_forest.decision_function(X_test)\n",
        "anomaly_labels = iso_forest.predict(X_test)\n",
        "\n",
        "# Thống kê\n",
        "n_anomalies = (anomaly_labels == -1).sum()\n",
        "print(f\"Số anomalies phát hiện: {n_anomalies} ({n_anomalies/len(X_test)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "D.3. ĐỀ XUẤT GIÁ TRỊ THAY THẾ\n",
            "============================================================\n",
            "Test đề xuất giá trị:\n",
            "\n",
            "PHA:\n",
            "  - EVN.PHA_3P (source: rule, conf: 1.0)\n",
            "  - EVN.PHA_3P (source: mode, conf: 0.8)\n",
            "\n",
            "KIEU_MC:\n",
            "  - TBI_CT_MC_KIEU_MC_01 (source: rule, conf: 1.0)\n",
            "  - TBI_CT_MC_KIEU_MC_01 (source: mode, conf: 0.8)\n",
            "\n",
            "OWNER:\n",
            "  - TB0632 (source: mode, conf: 0.8)\n"
          ]
        }
      ],
      "source": [
        "# D.3. Tự đề xuất giá trị thay thế\n",
        "print(\"=\" * 60)\n",
        "print(\"D.3. ĐỀ XUẤT GIÁ TRỊ THAY THẾ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class ValueSuggester:\n",
        "    \"\"\"Đề xuất giá trị thay thế cho các trường thiếu/lỗi\"\"\"\n",
        "    \n",
        "    def __init__(self, df, rules):\n",
        "        self.df = df\n",
        "        self.rules = rules\n",
        "        self.mode_values = {}\n",
        "        \n",
        "        # Tính mode cho mỗi cột\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                mode = df[col].mode()\n",
        "                if len(mode) > 0:\n",
        "                    self.mode_values[col] = mode[0]\n",
        "    \n",
        "    def suggest(self, column, current_value=None):\n",
        "        \"\"\"Đề xuất giá trị cho một cột\"\"\"\n",
        "        suggestions = []\n",
        "        \n",
        "        # Rule-based suggestion\n",
        "        if column in self.rules:\n",
        "            suggestions.append({\n",
        "                'value': self.rules[column],\n",
        "                'source': 'rule',\n",
        "                'confidence': 1.0\n",
        "            })\n",
        "        \n",
        "        # Mode-based suggestion\n",
        "        if column in self.mode_values:\n",
        "            suggestions.append({\n",
        "                'value': self.mode_values[column],\n",
        "                'source': 'mode',\n",
        "                'confidence': 0.8\n",
        "            })\n",
        "        \n",
        "        return suggestions\n",
        "\n",
        "value_suggester = ValueSuggester(train_df, NORMALIZATION_RULES)\n",
        "\n",
        "# Test suggestions\n",
        "print(\"Test đề xuất giá trị:\")\n",
        "for col in ['PHA', 'KIEU_MC', 'OWNER']:\n",
        "    if col in df.columns:\n",
        "        suggestions = value_suggester.suggest(col)\n",
        "        print(f\"\\n{col}:\")\n",
        "        for s in suggestions:\n",
        "            print(f\"  - {s['value']} (source: {s['source']}, conf: {s['confidence']})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.9.18)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
